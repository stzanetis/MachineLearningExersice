{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4b3947c-bd0b-4d98-9d5b-39b515a958c8",
   "metadata": {},
   "source": [
    "# Pattern Recognision and Machine Learning\n",
    "By **Tzanetis Savvas**(10889) and **Zoidis Vasilis**(10652).\n",
    "\n",
    "## Part D\n",
    "Once again, the first thing we need to do is import the correct **libraries**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b74199f9-901d-48f5-8e55-190e677390a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac8fb75-8adf-417c-a18e-53e2669084cc",
   "metadata": {},
   "source": [
    "The library **numpy** is used for exporting our results for the predictions on the **test** dataset, to a **.npy** files, while **pandas** is used for reading the datasets **Test** and **TV** from the **.csv** files. After that, we will also import some libraries from **sklearn**, such as **SVC**, **StandardScaler** and **cross_val_score**. We will explain the use of these libaries later.\n",
    "\n",
    "Next, we need to load the **TV** and **Test** datasets like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28e7df91-1171-4d08-8f9e-f3572af2387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"Datasets/datasetTV.csv\", header=None)\n",
    "test_data = pd.read_csv(\"Datasets/datasetTest.csv\", header=None)\n",
    "\n",
    "F_train = train_data.iloc[:, :-1].values\n",
    "L_train = train_data.iloc[:, -1].values\n",
    "\n",
    "F_test = test_data.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3dde6a-e520-48bd-80ef-f161ec82f88c",
   "metadata": {},
   "source": [
    "In the code above, apart from loading the datasets using **pandas**, we also seperate the features and labels from the **TV** dataset, and extract the features of the **Test** dataset.\n",
    "\n",
    "Next, using the aforementioned **StandardScaler** library, we will be scale our data. This ensures that all features contribute equally to the model's decision-making process, as features with larger scales won't be able to dominate the learning process. Also, scaling accelerates the convergence of gradient descent used in our model of choice the **SVM(Support Vector Machine)**, ensuring that the optimization algorithm moves at a consistent rate for all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "892c5633-7f1f-49bb-bc40-9c61591b0c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "F_train = scaler.fit_transform(F_train)\n",
    "F_test = scaler.transform(F_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a0f7e3-9e13-481d-9a56-c616fd684edb",
   "metadata": {},
   "source": [
    "All that's left now is to train our model, but before doing this we must first tune the **hyper-parameters** of our **Support Vector Machine**. This is done using **Grid Search** as well as **Random Search** when the computational load for calculating the best combination of parameters using the **Grid Search** method seems too expensive. With these things in mind, we found out that the best parameters are:\n",
    "\n",
    "- *kernel = **rbf***\n",
    "- *C = **4***\n",
    "- *gamma = **scale***\n",
    "- *random_state = **42***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e93c0846-c2e1-4180-a12e-0e39e8d91da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = SVC(kernel='rbf', C=4, gamma='scale', random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d02e482-9c8f-4b37-8bec-6056e38d3778",
   "metadata": {},
   "source": [
    "We also need to evaluate our model's accuracy using **5-fold cross-validation**, as this method helps us detect **overfitting** and gives us a better estimate on *unseen* data by splitting our dataset into **5** folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df82968d-5718-49ef-832c-2dc1eb4b1194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.864494   0.85877644 0.85134362 0.8506865  0.83981693]\n",
      "Cross-validation accuracy: 0.85\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(svm_model, F_train, L_train, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-validation scores: {scores}\")\n",
    "print(f\"Cross-validation accuracy: {scores.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92ed8ab-a2bc-4579-baa3-f0a8ecabe24f",
   "metadata": {},
   "source": [
    "Last but not least. We fit our model on the **Training** dataset and make predictions for the given **Test** dataset using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90833abf-6d55-442a-a098-22d5657cb2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model.fit(F_train, L_train)\n",
    "\n",
    "labelsX = svm_model.predict(F_test)\n",
    "np.save(\"Results/labelsX.npy\", labelsX)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
